{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9cb7107-f037-4bb5-9b67-5a1630c67ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c6dd2-1077-41bf-9239-fe00bc63acd7",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Install Spark and PySpark \n",
    "\n",
    "* Install Spark\n",
    "* Run PySpark\n",
    "* Create a local spark session\n",
    "* Execute spark.version.\n",
    "  \n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ff2351-3261-4830-8c29-a125bfca33e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/05 17:18:15 WARN Utils: Your hostname, codespaces-47ef95 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "24/03/05 17:18:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/05 17:18:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/05 17:18:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ce6b2b-53c7-47bb-a3cb-ac3aa9760e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f499313-79e6-499f-a172-1da720a659fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('fhv_tripdata_2019-10.csv.gz', nrows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bc2a1-7328-4733-a290-8104d75f83ac",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "FHV October 2019 \\\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\\\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d490bd-b879-4322-8c82-8cc768f34d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PUlocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOlocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.StringType(), True), \n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "    ])\n",
    "\n",
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3044bb2-df3c-4f4d-bef7-5a0b00ea6be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropOff_datetime', TimestampType(), True), StructField('PUlocationID', IntegerType(), True), StructField('DOlocationID', IntegerType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f87ff310-e0e0-4b81-96c2-0127972a2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.iteritems = pd.DataFrame.items\n",
    "# pd.DataFrame.iteritems = pd.DataFrame.items\n",
    "# spark.createDataFrame(df).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256e4794-fcbd-485a-9a0c-c4d8e8f8681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv \\\n",
    "    .repartition(6) \\\n",
    "    .write.parquet('fhvhv/2019/10/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ec2dcb-7fe2-4ba0-9db1-3f94e6d4d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('fhvhv/2019/10/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3238abd-3236-4a2a-8bae-53c27ed3a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c85557f-7c8e-4281-8826-5cb6e25eb0db",
   "metadata": {},
   "source": [
    "### Question 3:\r\n",
    "Count record\r\n",
    "\r\n",
    "How many taxi trips were there on the 15th of Octobr?\r\n",
    "\r\n",
    "Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbc6f44c-4c6d-45ab-a879-28bc39aea585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.createOrReplaceTempView('fhv_trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6801f6f9-79b9-464d-8346-6ea7bbe270c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*)\n",
    "FROM\n",
    "    fhv_trips_data\n",
    "WHERE pickup_datetime >= '2019-10-15 00:00:00' AND pickup_datetime <= '2019-10-15 23:59:59' \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ece81-d7a5-4902-9fa7-adff7aaee2ae",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "Longest trip for each day.\n",
    "What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a86cbde-965b-4a0b-b15a-9d3258503164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = df_fhv \\\n",
    "    .withColumn(\"trip_duration_hours\", (F.col(\"dropOff_datetime\").cast(\"long\") - F.col(\"pickup_datetime\").cast(\"long\")) / 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "983d3ce6-e0de-4ff7-8302-fa98ab317952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv.createOrReplaceTempView('fhv_trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e65a39c-c7d0-4155-a75e-dd364b3150ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|    pickup_datetime|   dropOff_datetime|trip_duration_hours|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2019-10-01 09:55:38|2019-10-01 10:05:43|0.16805555555555557|\n",
      "|2019-10-21 04:15:47|2019-10-21 04:36:04|0.33805555555555555|\n",
      "|2019-10-19 12:00:00|2019-10-19 12:20:00| 0.3333333333333333|\n",
      "|2019-10-11 14:28:00|2019-10-11 14:32:44|0.07888888888888888|\n",
      "|2019-10-21 18:00:26|2019-10-21 18:07:21|0.11527777777777778|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.select('pickup_datetime','dropOff_datetime', 'trip_duration_hours').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "717642fb-fe2c-41ff-b651-375272b41999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|trip_duration_hours|\n",
      "+-------------------+\n",
      "|           631152.5|\n",
      "|           631152.5|\n",
      "|  87672.44083333333|\n",
      "|  70128.02805555555|\n",
      "|             8794.0|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "     trip_duration_hours\n",
    "FROM\n",
    "    fhv_trips_data\n",
    "ORDER BY trip_duration_hours DESC\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2cd25-cc7e-42bc-9fcc-33f93ccc7ed0",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "User Interface\n",
    "Spark’s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "4040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35cf446-3bb4-4060-ad08-9f113b348dfc",
   "metadata": {},
   "source": [
    "### Question 6 \n",
    "\n",
    "Least frequent pickup location zone. Load the zone lookup data into a temp view in Spark Zone Data.\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone? Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "240e72c1-b60b-4a48-a152-20c68ad89ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d45e0ea-5e1e-4a27-bc49-9038d14ecbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1c1b633-8080-4edd-81f4-c9306948f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_zones = df_fhv.join(df_zone, df_fhv.PUlocationID == df_zone.LocationID)\n",
    "df_fhv_zones.createOrReplaceTempView('FHV_ZONES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccdb4c93-295d-41c3-98b5-9fdcb5434014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|       ZONE|TOTAL_PICKUPS|\n",
      "+-----------+-------------+\n",
      "|Jamaica Bay|            1|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "     ZONE, COUNT(*) AS TOTAL_PICKUPS\n",
    "FROM\n",
    "    FHV_ZONES\n",
    "GROUP BY ZONE\n",
    "ORDER BY TOTAL_PICKUPS\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
